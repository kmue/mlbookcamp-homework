{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22bd183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # for sqrt...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import train_test_split # for train test split\n",
    "from sklearn.metrics import mutual_info_score # for mutual information score\n",
    "from sklearn.metrics import accuracy_score # for accuracy score\n",
    "from sklearn.metrics import mean_squared_error # for q6\n",
    "from sklearn.feature_extraction import DictVectorizer # for one-hot encoding\n",
    "from sklearn.linear_model import LogisticRegression # for log reg\n",
    "from sklearn.linear_model import Ridge # for q6\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import roc_auc_score # for hw4 q1\n",
    "from matplotlib import pyplot as plt # for hw4 q3\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "891fd335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"AER_credit_card_data.csv\")\n",
    "# The goal of this homework is to inspect the output of different evaluation metrics \n",
    "# by creating a classification model (target column card)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d53b2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "# Create the target variable by mapping yes to 1 and no to 0.\n",
    "df['card'] = df['card'].map(dict(yes=1, no=0)) # <-- target\n",
    "df['owner'] = df['owner'].map(dict(yes=1, no=0))\n",
    "df['selfemp'] = df['selfemp'].map(dict(yes=1, no=0))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5402e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 3 parts: train/validation/test with 60%/20%/20% distribution. \n",
    "# Use train_test_split funciton for that with random_state=1.\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(\n",
    "    df.drop('card', axis = 1),\n",
    "    df[['card']],\n",
    "    train_size = 0.8,\n",
    "    test_size = 0.2,\n",
    "    random_state = 42)\n",
    "# print(df.shape)\n",
    "# print()\n",
    "# print(X_train_valid.shape)\n",
    "# print(y_train_valid.shape)\n",
    "# print()\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid,\n",
    "    y_train_valid,\n",
    "    train_size = 0.75,\n",
    "    test_size = 0.25,\n",
    "    shuffle = False)\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print()\n",
    "# print(X_valid.shape)\n",
    "# print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a7d667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7119481870960674\n",
      "0.5229214590864278\n",
      "0.5951363785737758\n",
      "0.9876446854346954\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "# ROC AUC could also be used to evaluate feature importance of numerical variables.\n",
    "# Let's do that\n",
    "\n",
    "# For each numerical variable, use it as score and compute AUC with the card variable.\n",
    "# Use the training dataset for that.\n",
    "# If your AUC is < 0.5, invert this variable by putting \"-\" in front\n",
    "roc_auc_reports = roc_auc_score(y_train, -X_train[['reports']])\n",
    "print(roc_auc_reports)\n",
    "\n",
    "roc_auc_dependents = roc_auc_score(y_train, -X_train[['dependents']])\n",
    "print(roc_auc_dependents)\n",
    "\n",
    "roc_auc_active = roc_auc_score(y_train, X_train[['active']])\n",
    "print(roc_auc_active)\n",
    "\n",
    "roc_auc_share = roc_auc_score(y_train, X_train[['share']])\n",
    "print(roc_auc_share)\n",
    "\n",
    "# numerical variables: reports, age, income, share, expenditure, dependents, months, active\n",
    "# relevant for question: reports, dependents, active, share\n",
    "\n",
    "# (e.g. -df_train['expenditure'])\n",
    "# AUC can go below 0.5 if the variable is negatively correlated with the target varialble. You can change the direction of the correlation by negating this variable - then negative correlation becomes positive.\n",
    "\n",
    "# Which numerical variable (among the following 4) has the highest AUC?\n",
    "# reports\n",
    "# dependents\n",
    "# active\n",
    "# --> share <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1c5d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "# From now on, use these columns only:\n",
    "columns_to_use = [\"reports\", \"age\", \"income\", \"share\", \"expenditure\", \"dependents\", \"months\", \"majorcards\", \"active\", \"owner\", \"selfemp\"]\n",
    "columns_to_use\n",
    "X_train_q2 = X_train[columns_to_use].copy()\n",
    "X_valid_q2 = X_valid[columns_to_use].copy()\n",
    "X_test_q2 = X_test[columns_to_use].copy()\n",
    "# Apply one-hot-encoding using DictVectorizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4207c507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reports</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>share</th>\n",
       "      <th>expenditure</th>\n",
       "      <th>dependents</th>\n",
       "      <th>months</th>\n",
       "      <th>majorcards</th>\n",
       "      <th>active</th>\n",
       "      <th>owner</th>\n",
       "      <th>selfemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0</td>\n",
       "      <td>43.58333</td>\n",
       "      <td>4.3681</td>\n",
       "      <td>0.044293</td>\n",
       "      <td>160.4792</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>0</td>\n",
       "      <td>28.08333</td>\n",
       "      <td>5.4700</td>\n",
       "      <td>0.029192</td>\n",
       "      <td>133.0683</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0</td>\n",
       "      <td>30.66667</td>\n",
       "      <td>3.8000</td>\n",
       "      <td>0.032519</td>\n",
       "      <td>102.9767</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>0</td>\n",
       "      <td>37.16667</td>\n",
       "      <td>1.6800</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>36.41667</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.113059</td>\n",
       "      <td>282.5633</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      reports       age  income     share  expenditure  dependents  months  \\\n",
       "598         0  43.58333  4.3681  0.044293     160.4792           2     110   \n",
       "1213        0  28.08333  5.4700  0.029192     133.0683           3      18   \n",
       "209         0  30.66667  3.8000  0.032519     102.9767           0      26   \n",
       "538         0  37.16667  1.6800  0.000714       0.0000           3     120   \n",
       "140         0  36.41667  3.0000  0.113059     282.5633           2      16   \n",
       "\n",
       "      majorcards  active  owner  selfemp  \n",
       "598            1      10      1        0  \n",
       "1213           1       0      1        0  \n",
       "209            0       1      0        0  \n",
       "538            0       8      1        0  \n",
       "140            1       6      0        0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_q2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd917192",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = X_train_q2.to_dict(orient='records')\n",
    "# print(train_dict)\n",
    "dv = DictVectorizer(sparse=False)\n",
    "dv.fit(train_dict)\n",
    "# print(dv.get_feature_names_out())\n",
    "X_train_q2 = dv.transform(train_dict)\n",
    "\n",
    "valid_dict = X_valid_q2.to_dict(orient='records')\n",
    "dv = DictVectorizer(sparse=False)\n",
    "dv.fit(valid_dict)\n",
    "X_valid_q2 = dv.transform(valid_dict)\n",
    "\n",
    "test_dict = X_test_q2.to_dict(orient='records')\n",
    "dv = DictVectorizer(sparse=False)\n",
    "dv.fit(test_dict)\n",
    "X_test_q2 = dv.transform(test_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caea9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and train the logistic regression with these parameters:\n",
    "model_q2 = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "model_q2.fit(X_train_q2, y_train.values.ravel())\n",
    "card_valid_pred_q2 = model_q2.predict(X_valid_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1b3f531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 9.99928982e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.72189274e-03, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.13906352e-01, 9.99999972e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.65021071e-01,\n",
       "       5.83192873e-05, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.67599689e-01, 1.00000000e+00, 4.61326505e-03, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 9.99999998e-01,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 2.64831943e-02,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 5.84551152e-05, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 3.21580363e-02, 2.51674504e-02,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 7.28528225e-07,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       7.98391436e-02, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.23978715e-01, 9.99999997e-01, 1.45349496e-01, 1.00000000e+00,\n",
       "       9.62468832e-01, 3.51577882e-01, 6.58443790e-06, 1.00000000e+00,\n",
       "       2.34612641e-02, 1.46178151e-02, 1.00000000e+00, 9.98047111e-01,\n",
       "       1.00000000e+00, 1.00000000e+00, 2.19832834e-01, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.38578028e-01, 1.00000000e+00,\n",
       "       3.48158962e-01, 3.22910639e-02, 1.00000000e+00, 1.00000000e+00,\n",
       "       9.99975019e-01, 1.00000000e+00, 1.00000000e+00, 1.87768752e-01,\n",
       "       1.00000000e+00, 1.00000000e+00, 2.72101338e-01, 9.79949747e-01,\n",
       "       8.69072009e-02, 1.00000000e+00, 1.48468565e-01, 1.00000000e+00,\n",
       "       1.00000000e+00, 5.02170884e-02, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.26164625e-01, 1.47295833e-01,\n",
       "       1.00000000e+00, 9.91696256e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.84251373e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 9.99413428e-01,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 9.95193351e-01,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.41641792e-04, 1.00000000e+00,\n",
       "       1.00000000e+00, 3.16239543e-03, 1.35386338e-01, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 3.12277608e-07, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 2.06955915e-01,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 3.70721244e-02,\n",
       "       1.00000000e+00, 1.18967854e-02, 1.00000000e+00, 1.00000000e+00,\n",
       "       3.38658059e-03, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       5.88563016e-02, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00308128e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       9.99996802e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       2.37346137e-03, 1.00000000e+00, 1.00000000e+00, 1.26843517e-02,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.65693190e-02, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.64342769e-01, 1.53144891e-02, 1.00000000e+00,\n",
       "       1.00000000e+00, 4.96563408e-02, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 8.89429639e-06, 1.00000000e+00,\n",
       "       7.38094379e-03, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 9.99961389e-01,\n",
       "       1.51246124e-02, 9.41659926e-03, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.56213846e-02, 1.00000000e+00, 4.66767714e-03, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 8.41650336e-05, 1.00000000e+00,\n",
       "       1.61519195e-01, 1.51731839e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 9.99999960e-01, 2.19749725e-02, 5.97925808e-02,\n",
       "       1.00000000e+00, 1.00000000e+00, 9.99999867e-01, 2.72820729e-02])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_valid_pred_proba = model_q2.predict_proba(X_valid_q2)[:, 1]\n",
    "card_valid_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af854919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.993"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2\n",
    "# What's the AUC of this model on the validation dataset? (round to 3 digits)\n",
    "\n",
    "round(roc_auc_score(y_valid, card_valid_pred_q2),3)\n",
    "\n",
    "# 0.615\n",
    "# 0.515\n",
    "# 0.715\n",
    "# --> 0.995 <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f852be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "# Now let's compute precision and recall for our model.\n",
    "# Evaluate the model on all thresholds from 0.0 to 1.0 with step 0.01\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "thresholds\n",
    "# For each threshold, compute precision and recall\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for t in thresholds:\n",
    "    true_positive = ((card_valid_pred_proba >= t) & (np.array(y_valid) == 1)).sum()\n",
    "    false_positive = ((card_valid_pred_proba >= t) & (np.array(y_valid) == 0)).sum()\n",
    "    false_negative = ((card_valid_pred_proba < t) & (np.array(y_valid) == 1)).sum()\n",
    "    true_negative = ((card_valid_pred_proba < t) & (np.array(y_valid) == 0)).sum()\n",
    "    prec = true_positive / (true_positive + false_positive)\n",
    "    reca = true_positive / (true_positive + false_negative)\n",
    "    precision.append(prec)\n",
    "    recall.append(reca)\n",
    "    #print('%0.2f %0.3f' % (prec, reca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db3a8dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.plot(thresholds, precision, color='blue')\n",
    "plt.plot(thresholds, recall, color='green')\n",
    "\n",
    "plt.title('Precision and Recall vs Threshold')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision and Recall')\n",
    "\n",
    "plt.xticks(np.linspace(0, 1, 11))\n",
    "\n",
    "# plt.savefig('04_threshold_accuracy.svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e296f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot them\n",
    "# At which threshold precision and recall curves intersect?\n",
    "# 0.1\n",
    "# --> 0.3 <--\n",
    "# 0.6\n",
    "# 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c8aca7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 0.884\n",
      "0.01 0.859\n",
      "0.02 0.848\n",
      "0.03 0.840\n",
      "0.04 0.835\n",
      "0.05 0.833\n",
      "0.06 0.828\n",
      "0.07 0.828\n",
      "0.08 0.826\n",
      "0.09 0.824\n",
      "0.10 0.824\n",
      "0.11 0.823\n",
      "0.12 0.821\n",
      "0.13 0.817\n",
      "0.14 0.814\n",
      "0.15 0.808\n",
      "0.16 0.807\n",
      "0.17 0.799\n",
      "0.18 0.799\n",
      "0.19 0.795\n",
      "0.20 0.795\n",
      "0.21 0.794\n",
      "0.22 0.792\n",
      "0.23 0.792\n",
      "0.24 0.792\n",
      "0.25 0.792\n",
      "0.26 0.792\n",
      "0.27 0.792\n",
      "0.28 0.790\n",
      "0.29 0.790\n",
      "0.30 0.790\n",
      "0.31 0.790\n",
      "0.32 0.790\n",
      "0.33 0.790\n",
      "0.34 0.790\n",
      "0.35 0.788\n",
      "0.36 0.786\n",
      "0.37 0.786\n",
      "0.38 0.786\n",
      "0.39 0.786\n",
      "0.40 0.786\n",
      "0.41 0.786\n",
      "0.42 0.786\n",
      "0.43 0.786\n",
      "0.44 0.786\n",
      "0.45 0.786\n",
      "0.46 0.786\n",
      "0.47 0.786\n",
      "0.48 0.786\n",
      "0.49 0.786\n",
      "0.50 0.786\n",
      "0.51 0.786\n",
      "0.52 0.786\n",
      "0.53 0.786\n",
      "0.54 0.786\n",
      "0.55 0.786\n",
      "0.56 0.786\n",
      "0.57 0.786\n",
      "0.58 0.786\n",
      "0.59 0.786\n",
      "0.60 0.786\n",
      "0.61 0.786\n",
      "0.62 0.786\n",
      "0.63 0.786\n",
      "0.64 0.786\n",
      "0.65 0.786\n",
      "0.66 0.786\n",
      "0.67 0.786\n",
      "0.68 0.786\n",
      "0.69 0.786\n",
      "0.70 0.786\n",
      "0.71 0.786\n",
      "0.72 0.786\n",
      "0.73 0.786\n",
      "0.74 0.786\n",
      "0.75 0.786\n",
      "0.76 0.786\n",
      "0.77 0.786\n",
      "0.78 0.786\n",
      "0.79 0.786\n",
      "0.80 0.786\n",
      "0.81 0.786\n",
      "0.82 0.786\n",
      "0.83 0.786\n",
      "0.84 0.786\n",
      "0.85 0.786\n",
      "0.86 0.786\n",
      "0.87 0.786\n",
      "0.88 0.786\n",
      "0.89 0.786\n",
      "0.90 0.786\n",
      "0.91 0.786\n",
      "0.92 0.786\n",
      "0.93 0.786\n",
      "0.94 0.786\n",
      "0.95 0.786\n",
      "0.96 0.786\n",
      "0.97 0.784\n",
      "0.98 0.782\n",
      "0.99 0.782\n",
      "1.00 0.743\n"
     ]
    }
   ],
   "source": [
    "# Question 4\n",
    "# Precision and recall are conflicting - when one grows, the other goes down. \n",
    "# That's why they are often combined into the F1 score - a metrics that takes into account both\n",
    "# This is the formula for computing F1:\n",
    "# F1 = 2 * P * R / (P + R)\n",
    "# Where P is precision and R is recall.\n",
    "# Let's compute F1 for all thresholds from 0.0 to 1.0 with increment 0.01\n",
    "f1 = []\n",
    "for t in thresholds:\n",
    "    true_positive = ((card_valid_pred_proba >= t) & (np.array(y_valid) == 1)).sum()\n",
    "    false_positive = ((card_valid_pred_proba >= t) & (np.array(y_valid) == 0)).sum()\n",
    "    false_negative = ((card_valid_pred_proba < t) & (np.array(y_valid) == 1)).sum()\n",
    "    true_negative = ((card_valid_pred_proba < t) & (np.array(y_valid) == 0)).sum()\n",
    "    prec = true_positive / (true_positive + false_positive)\n",
    "    reca = true_positive / (true_positive + false_negative)\n",
    "    #precision.append(prec)\n",
    "    #recall.append(reca)\n",
    "    f1_t = 2 * prec * reca / ( prec + reca )\n",
    "    f1.append(f1_t)\n",
    "    print('%0.2f %0.3f' % (t, f1_t))\n",
    "\n",
    "# At which threshold F1 is maximal?\n",
    "# --> 0.1 <-- # I guess?\n",
    "# 0.4\n",
    "# 0.6\n",
    "# 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b840ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5\n",
    "# Use the KFold class from Scikit-Learn to evaluate our model on 5 different folds:\n",
    "# KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Iterate over different folds of df_full_train\n",
    "# Split the data into train and validation\n",
    "# Train the model on train with these parameters: LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "# Use AUC to evaluate the model on validation\n",
    "# How large is standard devidation of the AUC scores across different folds?\n",
    "# 0.003\n",
    "# 0.014\n",
    "# 0.09\n",
    "# 0.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd36c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6\n",
    "# Now let's use 5-Fold cross-validation to find the best parameter C\n",
    "# Iterate over the following C values: [0.01, 0.1, 1, 10]\n",
    "# Initialize KFold with the same parameters as previously\n",
    "# Use these parametes for the model: LogisticRegression(solver='liblinear', C=C, max_iter=1000)\n",
    "# Compute the mean score as well as the std (round the mean and std to 3 decimal digits)\n",
    "# Which C leads to the best mean score?\n",
    "# 0.01\n",
    "# 0.1\n",
    "# 1\n",
    "# 10\n",
    "# If you have ties, select the score with the lowest std. If you still have ties, select the smallest C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
