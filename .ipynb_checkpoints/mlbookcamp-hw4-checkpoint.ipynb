{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22bd183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # for sqrt...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import train_test_split # for train test split\n",
    "from sklearn.metrics import mutual_info_score # for mutual information score\n",
    "from sklearn.metrics import accuracy_score # for accuracy score\n",
    "from sklearn.metrics import mean_squared_error # for q6\n",
    "from sklearn.feature_extraction import DictVectorizer # for one-hot encoding\n",
    "from sklearn.linear_model import LogisticRegression # for log reg\n",
    "from sklearn.linear_model import Ridge # for q6\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "891fd335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"AER_credit_card_data.csv\")\n",
    "# The goal of this homework is to inspect the output of different evaluation metrics \n",
    "# by creating a classification model (target column card)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d53b2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "# Create the target variable by mapping yes to 1 and no to 0.\n",
    "# Split the dataset into 3 parts: train/validation/test with 60%/20%/20% distribution. \n",
    "# Use train_test_split funciton for that with random_state=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22a7d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "# ROC AUC could also be used to evaluate feature importance of numerical variables.\n",
    "# Let's do that\n",
    "\n",
    "# For each numerical variable, use it as score and compute AUC with the card variable.\n",
    "# Use the training dataset for that.\n",
    "# If your AUC is < 0.5, invert this variable by putting \"-\" in front\n",
    "\n",
    "# (e.g. -df_train['expenditure'])\n",
    "# AUC can go below 0.5 if the variable is negatively correlated with the target varialble. You can change the direction of the correlation by negating this variable - then negative correlation becomes positive.\n",
    "\n",
    "# Which numerical variable (among the following 4) has the highest AUC?\n",
    "# reports\n",
    "# dependents\n",
    "# active\n",
    "# share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "# From now on, use these columns only:\n",
    "# [\"reports\", \"age\", \"income\", \"share\", \"expenditure\", \"dependents\", \"months\", \"majorcards\", \"active\", \"owner\", \"selfemp\"]\n",
    "# Apply one-hot-encoding using DictVectorizer and train the logistic regression with these parameters:\n",
    "# LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af854919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "# What's the AUC of this model on the validation dataset? (round to 3 digits)\n",
    "# 0.615\n",
    "# 0.515\n",
    "# 0.715\n",
    "# 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f852be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "# Now let's compute precision and recall for our model.\n",
    "# Evaluate the model on all thresholds from 0.0 to 1.0 with step 0.01\n",
    "# For each threshold, compute precision and recall\n",
    "# Plot them\n",
    "# At which threshold precision and recall curves intersect?\n",
    "# 0.1\n",
    "# 0.3\n",
    "# 0.6\n",
    "# 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8aca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4\n",
    "# Precision and recall are conflicting - when one grows, the other goes down. That's why they are often combined into the F1 score - a metrics that takes into account both\n",
    "# This is the formula for computing F1:\n",
    "# F1 = 2 * P * R / (P + R)\n",
    "# Where P is precision and R is recall.\n",
    "# Let's compute F1 for all thresholds from 0.0 to 1.0 with increment 0.01\n",
    "# At which threshold F1 is maximal?\n",
    "# 0.1\n",
    "# 0.4\n",
    "# 0.6\n",
    "# 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b840ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5\n",
    "# Use the KFold class from Scikit-Learn to evaluate our model on 5 different folds:\n",
    "# KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Iterate over different folds of df_full_train\n",
    "# Split the data into train and validation\n",
    "# Train the model on train with these parameters: LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "# Use AUC to evaluate the model on validation\n",
    "# How large is standard devidation of the AUC scores across different folds?\n",
    "# 0.003\n",
    "# 0.014\n",
    "# 0.09\n",
    "# 0.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd36c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6\n",
    "# Now let's use 5-Fold cross-validation to find the best parameter C\n",
    "# Iterate over the following C values: [0.01, 0.1, 1, 10]\n",
    "# Initialize KFold with the same parameters as previously\n",
    "# Use these parametes for the model: LogisticRegression(solver='liblinear', C=C, max_iter=1000)\n",
    "# Compute the mean score as well as the std (round the mean and std to 3 decimal digits)\n",
    "# Which C leads to the best mean score?\n",
    "# 0.01\n",
    "# 0.1\n",
    "# 1\n",
    "# 10\n",
    "# If you have ties, select the score with the lowest std. If you still have ties, select the smallest C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
